{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(object):\n",
    "  def __init__(self, w, x, y):\n",
    "    self.w = w\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "    \n",
    "\n",
    "  def error(self):   #define error function\n",
    "    sigma = 1/(1+np.exp(np.asarray(-np.dot(self.x, self.w), float)))\n",
    "    # pdb.set_trace()\n",
    "    er = -np.sum( np.dot ( self.y , np.log(sigma)) + np.dot((1 - self.y),np.log(1 - sigma)))\n",
    "    return er\n",
    "\n",
    "  def gradient(self):\n",
    "    # Computes the gradient of the log-likelyhood function\n",
    "    \n",
    "    return np.dot(self.x.T, 1/(1 + np.exp(np.asarray(-np.dot(self.x,self.w),float))) - self.y)\n",
    "    \n",
    "  \n",
    "  def Fit(self): #find optimized weights\n",
    "    a_k = 0.001  #constant rate number\n",
    "    error_list = []\n",
    "\n",
    "    #for k in range (0,10000): #gradient descent \n",
    "    error_last = self.error()+1\n",
    "    eta = 10e-5\n",
    "\n",
    "    while error_last - self.error() > eta :\n",
    "      error_last=self.error()\n",
    "      w_new = self.w - a_k * self.gradient()  #update rule\n",
    "      self.w = w_new\n",
    "      error_list.append(error_last)\n",
    "      \n",
    "    return self.w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
